{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOA6jG96z64BNPyy2evFJRC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyans-sureja/llm-101/blob/main/part8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self attention with trainable weights"
      ],
      "metadata": {
        "id": "owny3m0WPrS1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self attention mechanism also called as \"scaled dot-product attention\"\n",
        "\n",
        "1. we want to compute context vectors as weighted sum over the input vectors specific to a certain input element.\n",
        "2. we will introduce weight matrices that are updated during model training.\n",
        "3. These trainable weight matrices are crucial so that model can learn to produce good context vectors.\n",
        "4. We will implement the self attention mechanism step by step by introducing 3 trainable weight matrices: Wq, Wk, Wv (query, key and value)\n"
      ],
      "metadata": {
        "id": "A9qVr4NcPx4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "convert input embeddings into key, query and value vectors.\n",
        "\n",
        "sentence = your journey starts with one step. \\\n",
        "dimension = 3d \\\n",
        "input is 6x3 matrix.\n",
        "\n",
        "we will use Wq, Wk, and Wv of 3x2 random value initalized matrices.\n",
        "\n",
        "**Queries = Inputs * Wq [6x2 matrix]**\n",
        "\n",
        "**Keys = Inputs * Wk [6x2 matrix]**\n",
        "\n",
        "**Values = Inputs * Wv [6x2 matrix]**\n",
        "\n",
        "So every raw here represents one input token."
      ],
      "metadata": {
        "id": "ScecVc0s8RDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")"
      ],
      "metadata": {
        "id": "EPxbg-yX-FXv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that in GPT-like models, the input and output dimensions are usually the same.\n",
        "\n",
        "But for illustration purposes, to better follow the computation, we choose different input (d_in=3)\n",
        "and output (d_out=2) dimensions here."
      ],
      "metadata": {
        "id": "NchNCkDp-RPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_2 = inputs[1] #A\n",
        "d_in = inputs.shape[1] #B\n",
        "d_out = 2 #C"
      ],
      "metadata": {
        "id": "g_RBiqEO-sw9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
      ],
      "metadata": {
        "id": "8QDCrkKq-Qpe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(W_query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohrB4j0b-KDF",
        "outputId": "369d16e9-b55a-49e9-fa31-2b49f3fdaad7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[0.2961, 0.5166],\n",
            "        [0.2517, 0.6886],\n",
            "        [0.0740, 0.8665]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we are setting requires_grad=False to reduce clutter in the outputs for\n",
        "illustration purposes.\n",
        "\n",
        "If we were to use the weight matrices for model training, we\n",
        "would set requires_grad=True to update these matrices during model training.\n"
      ],
      "metadata": {
        "id": "M0JN_nLoATuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KM21-1BQAOOe",
        "outputId": "d645a746-63f6-4cba-d427-8329e869b796"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.5500, 0.8700, 0.6600])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_2 = x_2 @ W_query\n",
        "key_2 = x_2 @ W_key\n",
        "value_2 = x_2 @ W_value"
      ],
      "metadata": {
        "id": "pHl2-knbAEdj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(query_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWrWbn7wBE29",
        "outputId": "e9cc90ab-ec5d-4ea6-fe96-2d9addac43c2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4306, 1.4551])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keys = inputs @ W_key\n",
        "values = inputs @ W_value\n",
        "queries = inputs @ W_query"
      ],
      "metadata": {
        "id": "ASGswBiTBGZW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(keys)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwzdfyTwCWws",
        "outputId": "ab6e9f58-87e1-42c0-e5ff-34e734e254bb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3669, 0.7646],\n",
            "        [0.4433, 1.1419],\n",
            "        [0.4361, 1.1156],\n",
            "        [0.2408, 0.6706],\n",
            "        [0.1827, 0.3292],\n",
            "        [0.3275, 0.9642]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next step - compute the attention scores.\n",
        "\n",
        "we will start by computing only one context vector Z2. for attention score we will use query vector for the token to find how much it get attention from different tokens. To get that we can do dot product b/w query vector for token and key vectors of all other tokens.\n",
        "\n",
        "\n",
        "Query_2 = [1x2]\n",
        "\n",
        "keys = [6x2]\n",
        "\n",
        "queries * keys.transpose = [1x6]"
      ],
      "metadata": {
        "id": "2Cnua0fqC_DO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keys_2 = keys[1]\n",
        "print(keys_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rj9WneW_CXzU",
        "outputId": "f7540a5c-a2e7-4232-a4d3-7a39e67f7d61"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4433, 1.1419])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores_2 = query_2 @ keys.T\n",
        "print(attn_scores_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARQLbSw0Ehf7",
        "outputId": "7951a11f-1e02-49c5-a8d6-8abfa8308ea3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores = queries @ keys.T\n",
        "print(attn_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgmxjCkVEqfw",
        "outputId": "4cffb55f-b2dc-43e0-8bdd-88ab0639c933"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9231, 1.3545, 1.3241, 0.7910, 0.4032, 1.1330],\n",
            "        [1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
            "        [1.2544, 1.8284, 1.7877, 1.0654, 0.5508, 1.5238],\n",
            "        [0.6973, 1.0167, 0.9941, 0.5925, 0.3061, 0.8475],\n",
            "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707, 0.7307],\n",
            "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.softmax(attn_scores, dim=-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NI_N1b91E5Cj",
        "outputId": "04d95ee4-146f-47b5-fb25-b7e6c14ea9ac"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1484, 0.2285, 0.2217, 0.1301, 0.0883, 0.1831],\n",
            "        [0.1401, 0.2507, 0.2406, 0.1157, 0.0687, 0.1842],\n",
            "        [0.1406, 0.2496, 0.2397, 0.1164, 0.0696, 0.1841],\n",
            "        [0.1548, 0.2130, 0.2083, 0.1394, 0.1047, 0.1799],\n",
            "        [0.1577, 0.2067, 0.2028, 0.1428, 0.1122, 0.1777],\n",
            "        [0.1494, 0.2267, 0.2202, 0.1310, 0.0901, 0.1825]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We compute the attention weights by scaling the\n",
        "attention scores and using the softmax function we used earlier.\n",
        "\n",
        "The difference to earlier is\n",
        "that we now scale the attention scores by dividing them by the **square root of the\n",
        "embedding dimension of the keys.**\n",
        "\n",
        "Note that taking the square root is mathematically the\n",
        "same as exponentiating by 0.5"
      ],
      "metadata": {
        "id": "YiGsY0aBF45Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_k = keys.shape[-1]\n",
        "print(d_k)\n",
        "\n",
        "attn_weights = torch.softmax(attn_scores / d_k**0.5, dim=-1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIuuzhWfFgSm",
        "outputId": "c737d3f7-2488-482b-f198-33711ab63d89"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "tensor([[0.1551, 0.2104, 0.2059, 0.1413, 0.1074, 0.1799],\n",
            "        [0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820],\n",
            "        [0.1503, 0.2256, 0.2192, 0.1315, 0.0914, 0.1819],\n",
            "        [0.1591, 0.1994, 0.1962, 0.1477, 0.1206, 0.1769],\n",
            "        [0.1610, 0.1949, 0.1923, 0.1501, 0.1265, 0.1752],\n",
            "        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why divide by sqrt(dimension)\n",
        "\n",
        "- softmax function is sensitive to the magnitudes of its inputs. When the inputs are large, the difference between the exponential values of each input become much more pronounced. This causes the softmax output to become \"peaky\", where the highest value recieves almost all the probablity mass and the rest receives very little.\n",
        "\n",
        "check example in below cell."
      ],
      "metadata": {
        "id": "BpdpGxLjGuNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### experimentation cell\n",
        "\n",
        "tensor = torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])\n",
        "\n",
        "print(\"softmax without scalling: \", torch.softmax(tensor, dim=-1))\n",
        "\n",
        "scaled_tensor = tensor * 8\n",
        "print(\"softmax with scalling:\", torch.softmax(scaled_tensor, dim=-1))\n",
        "\n",
        "# see the results, few cells get almost all the weightage."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RsXjKflGLot",
        "outputId": "f8fa6390-d90e-4962-cdee-e43e0575cb7a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "softmax without scalling:  tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
            "softmax with scalling: tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the results in above example, model can become too much confident in one key. Such sharp distribution can make learning unstable."
      ],
      "metadata": {
        "id": "ozhgW24EIli8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### But why sqrt?\n",
        "\n",
        "To make the variance of the dot product scale.\n",
        "\n",
        "The dot product of Q and K increases the variance because multiplying two random numbers increases the variance.\n",
        "\n",
        "The increases in variance grows with dimension. Diving by sqrt(dimensio) keeps the variance close to 1.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "ZoRZmVcXI92b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# function to compute variance before and after scaling\n",
        "def compute_variance(dim, num_trials=1000):\n",
        "  dot_products = []\n",
        "  scaled_dot_products = []\n",
        "\n",
        "  for _ in range(num_trials):\n",
        "    q = np.random.randn(dim)\n",
        "    k = np.random.randn(dim)\n",
        "\n",
        "    dot_product = np.dot(q, k)\n",
        "    dot_products.append(dot_product)\n",
        "\n",
        "    scaled_dot_product = dot_product / np.sqrt(dim)\n",
        "    scaled_dot_products.append(scaled_dot_product)\n",
        "\n",
        "  variance_before_scalling = np.var(dot_products)\n",
        "  variance_after_scalling = np.var(scaled_dot_products)\n",
        "\n",
        "  return variance_before_scalling, variance_after_scalling\n",
        "\n",
        "\n",
        "# for dimension 5\n",
        "variance_before_5, variance_after_5 = compute_variance(5)\n",
        "print(f\"variance before scaling for dim=5: {variance_before_5}\")\n",
        "print(f\"variance after scaling for dim=5: {variance_after_5}\")\n",
        "\n",
        "\n",
        "# for dimension 20\n",
        "variance_before_20, variance_after_20 = compute_variance(20)\n",
        "print(f\"variance before scaling for dim=20: {variance_before_20}\")\n",
        "print(f\"variance after scaling for dim=20: {variance_after_20}\")\n",
        "\n",
        "\n",
        "# for dimension 100\n",
        "variance_before_100, variance_after_100 = compute_variance(100)\n",
        "print(f\"variance before scaling for dim=100: {variance_before_100}\")\n",
        "print(f\"variance after scaling for dim=100: {variance_after_100}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbFEVgQdITbS",
        "outputId": "31d4d657-a14e-4333-e789-3a5cec29d356"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "variance before scaling for dim=5: 5.656960453679603\n",
            "variance after scaling for dim=5: 1.1313920907359205\n",
            "variance before scaling for dim=20: 20.839055222643307\n",
            "variance after scaling for dim=20: 1.0419527611321655\n",
            "variance before scaling for dim=100: 95.87793641120774\n",
            "variance after scaling for dim=100: 0.9587793641120772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final step: compute the context vectors.\n",
        "\n",
        "we got attention weights using key and query vectors. using value vectors and this attention weights we can get the final context vectors.\n",
        "\n"
      ],
      "metadata": {
        "id": "Cu8DH80LMzfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_vec = attn_weights @ values\n",
        "print(context_vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOQbLppIJ2cf",
        "outputId": "875078ec-3f7a-4c76-9945-8f13d36e29df"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2996, 0.8053],\n",
            "        [0.3061, 0.8210],\n",
            "        [0.3058, 0.8203],\n",
            "        [0.2948, 0.7939],\n",
            "        [0.2927, 0.7891],\n",
            "        [0.2990, 0.8040]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing a compact self attention python class"
      ],
      "metadata": {
        "id": "Vb5D4j9qSDxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention_v1(nn.Module):\n",
        "\n",
        "  def __init__(self, d_in, d_out):\n",
        "    super().__init__()\n",
        "    self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
        "    self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
        "    self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
        "\n",
        "  def forward(self, x):\n",
        "    keys = x @ self.W_key\n",
        "    queries = x @ self.W_query\n",
        "    values = x @ self.W_value\n",
        "\n",
        "    attn_scores = queries @ keys.T\n",
        "    attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "PGbFFDYmR564"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this PyTorch code, SelfAttention_v1 is a class derived from nn.Module, which is a\n",
        "fundamental building block of PyTorch models, which provides necessary functionalities for\n",
        "model layer creation and management.    \n",
        "\n",
        "The __init__ method initializes trainable weight matrices (W_query, W_key, and\n",
        "W_value) for queries, keys, and values, each transforming the input dimension d_in to an\n",
        "output dimension d_out.\n",
        "\n",
        "During the forward pass, using the forward method, we compute the attention scores\n",
        "(attn_scores) by multiplying queries and keys, normalizing these scores using softmax.\n",
        "\n",
        "Finally, we create a context vector by weighting the values with these normalized attention\n"
      ],
      "metadata": {
        "id": "GC28BNovTMQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
        "print(sa_v1(inputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6DA2_UpTLnu",
        "outputId": "6ce092a3-6e0b-4a7d-b715-a5cb20112b36"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2996, 0.8053],\n",
            "        [0.3061, 0.8210],\n",
            "        [0.3058, 0.8203],\n",
            "        [0.2948, 0.7939],\n",
            "        [0.2927, 0.7891],\n",
            "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can improve the SelfAttention_v1 implementation further by utilizing PyTorch's\n",
        "nn.Linear layers, which effectively perform matrix multiplication when the bias units are\n",
        "disabled.\n",
        "\n",
        "Additionally, a significant advantage of using nn.Linear instead of manually\n",
        "implementing nn.Parameter(torch.rand(...)) is that nn.Linear has an optimized weight\n",
        "initialization scheme, contributing to more stable and effective model training.\n"
      ],
      "metadata": {
        "id": "Nlb3GPGbT7zp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention_v2(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.T\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec"
      ],
      "metadata": {
        "id": "xu-f77iuTLEP"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(789)\n",
        "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
        "print(sa_v2(inputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gt7fJTAWUGVT",
        "outputId": "135afd9a-ea8b-4d1a-8ef4-8c72bb7eb472"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0739,  0.0713],\n",
            "        [-0.0748,  0.0703],\n",
            "        [-0.0749,  0.0702],\n",
            "        [-0.0760,  0.0685],\n",
            "        [-0.0763,  0.0679],\n",
            "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that SelfAttention_v1 and SelfAttention_v2 give different outputs because they\n",
        "use different initial weights for the weight matrices since nn.Linear uses a more\n",
        "sophisticated weight initialization scheme."
      ],
      "metadata": {
        "id": "5yUMpPYoUVVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why do we use the terms key, query and value?\n",
        "\n",
        "Query = Analogous to search query in a database. It represents the current token the model focus on.\n",
        "\n",
        "Key = In attention mechanism, each item in input sequence has a key. Keys are used to match with the query.\n",
        "\n",
        "Value = It represents the actual content or representation of the input items. Once the model determines which keys(which parts of the input) are most relevant to the query(current focus item), it retrieves the corresponding values."
      ],
      "metadata": {
        "id": "inKsM9bOU8YG"
      }
    }
  ]
}