{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNcIVP6wn8LB6jEGXtRtOVD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyans-sureja/llm-101/blob/main/33_instruction_training_for_higher_epoch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Disclaimer : Previous note with just epoch = 5 change"
      ],
      "metadata": {
        "id": "92gucJ5vqg_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM instruction fine-tuning loop"
      ],
      "metadata": {
        "id": "WxeUa35qZErN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code from previous note"
      ],
      "metadata": {
        "id": "TfwakuBphltJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a16d5RywY9Kg",
        "outputId": "8b2455cb-b19c-42fa-af7c-4627a756a6ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of entries: 1100\n",
            "Training set length: 935\n",
            "Validation set length: 55\n",
            "Test set length: 110\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import urllib\n",
        "import ssl\n",
        "\n",
        "def download_and_load_file(file_path, url):\n",
        "    ssl_context = ssl.create_default_context()\n",
        "    ssl_context.check_hostname = False\n",
        "    ssl_context.verify_mode = ssl.CERT_NONE\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        with urllib.request.urlopen(url, context=ssl_context) as response:\n",
        "            text_data = response.read().decode(\"utf-8\")\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(text_data)\n",
        "    else:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            text_data = file.read()\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "file_path = \"instruction-data.json\"\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
        "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
        ")\n",
        "\n",
        "data = download_and_load_file(file_path, url)\n",
        "print(\"Number of entries:\", len(data))\n",
        "\n",
        "train_portion = int(len(data) * 0.85)  # 85% for training\n",
        "test_portion = int(len(data) * 0.1)    # 10% for testing\n",
        "val_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation\n",
        "\n",
        "train_data = data[:train_portion]\n",
        "test_data = data[train_portion:train_portion + test_portion]\n",
        "val_data = data[train_portion + test_portion:]\n",
        "\n",
        "print(\"Training set length:\", len(train_data))\n",
        "print(\"Validation set length:\", len(val_data))\n",
        "print(\"Test set length:\", len(test_data))\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class InstructionDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "\n",
        "        # Pre-tokenize texts\n",
        "        self.encoded_texts = []\n",
        "        for entry in data:\n",
        "            instruction_plus_input = self._format_input(entry)\n",
        "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
        "            full_text = instruction_plus_input + response_text\n",
        "            self.encoded_texts.append(\n",
        "                tokenizer.encode(full_text)\n",
        "            )\n",
        "\n",
        "    def _format_input(self, entry):\n",
        "      instruction_text = (\n",
        "          f\"Below is an instruction that describes a task. \"\n",
        "          f\"Write a response that appropriately completes the request.\"\n",
        "          f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
        "      )\n",
        "\n",
        "      input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
        "      return instruction_text + input_text\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.encoded_texts[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "def custom_collate_fn(\n",
        "    batch,\n",
        "    pad_token_id=50256,\n",
        "    ignore_index=-100,\n",
        "    allowed_max_length=None,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    # Find the longest sequence in the batch\n",
        "    batch_max_length = max(len(item)+1 for item in batch)\n",
        "\n",
        "    # Pad and prepare inputs and targets\n",
        "    inputs_lst, targets_lst = [], []\n",
        "\n",
        "    for item in batch:\n",
        "        new_item = item.copy()\n",
        "        # Add an <|endoftext|> token\n",
        "        new_item += [pad_token_id]\n",
        "        # Pad sequences to max_length\n",
        "        padded = (\n",
        "            new_item + [pad_token_id] *\n",
        "            (batch_max_length - len(new_item))\n",
        "        )\n",
        "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
        "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
        "\n",
        "        # New: Replace all but the first padding tokens in targets by ignore_index\n",
        "        mask = targets == pad_token_id\n",
        "        indices = torch.nonzero(mask).squeeze()\n",
        "        if indices.numel() > 1:\n",
        "            targets[indices[1:]] = ignore_index\n",
        "\n",
        "        # New: Optionally truncate to maximum sequence length\n",
        "        if allowed_max_length is not None:\n",
        "            inputs = inputs[:allowed_max_length]\n",
        "            targets = targets[:allowed_max_length]\n",
        "\n",
        "        inputs_lst.append(inputs)\n",
        "        targets_lst.append(targets)\n",
        "\n",
        "    # Convert list of inputs and targets to tensors and transfer to target device\n",
        "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "    targets_tensor = torch.stack(targets_lst).to(device)\n",
        "\n",
        "    return inputs_tensor, targets_tensor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loaders"
      ],
      "metadata": {
        "id": "gj52z0fYUKBW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The custom_collate_fn includes code to move the input and target tensors (for\n",
        "example, torch.stack(inputs_lst).to(device)) to a specified device, which can be\n",
        "either \"cpu\" or \"cuda\" (for GPUs), or optionally \"mps\" for Macs with Apple Silicon chips.\n",
        "\n",
        "In previous chapters, we moved the data onto the target device (for example, the GPU memory when device=\"cuda\") in the main training loop. Having this as part of the collate function offers the advantage of performing this device transfer process as a background\n",
        "process outside the training loop, preventing it from blocking the GPU during model training.\n"
      ],
      "metadata": {
        "id": "uKMXmcdrVtoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Note:\n",
        "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
        "# which is much faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
        "# However, the resulting loss values may be slightly different.\n",
        "\n",
        "#if torch.cuda.is_available():\n",
        "#    device = torch.device(\"cuda\")\n",
        "#elif torch.backends.mps.is_available():\n",
        "#    device = torch.device(\"mps\")\n",
        "#else:\n",
        "#    device = torch.device(\"cpu\")\n",
        "\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJ8rWHzmUDg7",
        "outputId": "82cccc5b-8686-4810-a3c4-1a99e8a780aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, to reuse the chosen device setting in custom_collate_fn when we plug it into the PyTorch DataLoader class later in this section, we use the partial function from Python's functools standard library to create a new version of the function with the device\n",
        "argument pre-filled.\n",
        "\n",
        "Additionally, we set the allowed_max_length to 1024, which truncates\n",
        "the data to the maximum context length supported by the GPT-2 model we finetune later in this chapter:"
      ],
      "metadata": {
        "id": "vdkfCNGGWbuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "customized_collate_fn = partial(custom_collate_fn, device=device, allowed_max_length=1024)"
      ],
      "metadata": {
        "id": "P49qD8JjWLRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 8\n",
        "\n",
        "torch.manual_seed(123)\n",
        "torch.cuda.manual_seed(123)\n",
        "\n",
        "train_dataset = InstructionDataset(train_data, tokenizer)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "val_dataset = InstructionDataset(val_data, tokenizer)\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "test_dataset = InstructionDataset(test_data, tokenizer)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")"
      ],
      "metadata": {
        "id": "03skBKKiWlKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train loader:\")\n",
        "for inputs, targets in train_loader:\n",
        "    print(inputs.shape, targets.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NcTsKNpXEQU",
        "outputId": "2f467ba6-ea3a-4510-cfe3-c5efa1592742"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader:\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 79]) torch.Size([8, 79])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 59]) torch.Size([8, 59])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 89]) torch.Size([8, 89])\n",
            "torch.Size([8, 59]) torch.Size([8, 59])\n",
            "torch.Size([8, 88]) torch.Size([8, 88])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 58]) torch.Size([8, 58])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 87]) torch.Size([8, 87])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 57]) torch.Size([8, 57])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 81]) torch.Size([8, 81])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 82]) torch.Size([8, 82])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 78]) torch.Size([8, 78])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the preceding output, we can see that the first input and target batch have dimensions 8×61, where 8 represents the batch size, and 61 is the number of tokens in each training example in this batch.\n",
        "\n",
        "The second input and target batch have a different number of\n",
        "tokens, for instance, 76.\n",
        "\n",
        "That is because in second batch max_length was 76 and in first batch it must be 61, hence different shapes."
      ],
      "metadata": {
        "id": "fKRk_4BGWK57"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next Step: Loading a pretrained LLM"
      ],
      "metadata": {
        "id": "jYHGQxaDYSOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model code from previous notes."
      ],
      "metadata": {
        "id": "KmnR8d3fYeX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "\n",
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "torch.manual_seed(123)\n",
        "torch.cuda.manual_seed_all(123)\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "    class LayerNorm(nn.Module):\n",
        "      def __init__(self, emb_dim):\n",
        "          super().__init__()\n",
        "          self.eps = 1e-5\n",
        "          self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "          self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "      def forward(self, x):\n",
        "          mean = x.mean(dim=-1, keepdim=True)\n",
        "          var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "          norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "          return self.scale * norm_x + self.shift\n",
        "\n",
        "    class GELU(nn.Module):\n",
        "      def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "      def forward(self, x):\n",
        "        # approx GeLu function\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "    class FeedForward(nn.Module):\n",
        "      def __init__(self, cfg):\n",
        "          super().__init__()\n",
        "          self.layers = nn.Sequential(\n",
        "              nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "              GPTModel.GELU(),\n",
        "              nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "          )\n",
        "\n",
        "      def forward(self, x):\n",
        "          return self.layers(x)\n",
        "\n",
        "    class MultiHeadAttention(nn.Module):\n",
        "      def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "          super().__init__()\n",
        "          assert (d_out % num_heads == 0), \\\n",
        "              \"d_out must be divisible by num_heads\"\n",
        "\n",
        "          self.d_out = d_out\n",
        "          self.num_heads = num_heads\n",
        "          self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "          self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "          self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "          self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "          self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "          self.dropout = nn.Dropout(dropout)\n",
        "          self.register_buffer(\n",
        "              \"mask\",\n",
        "              torch.triu(torch.ones(context_length, context_length),\n",
        "                        diagonal=1)\n",
        "          )\n",
        "\n",
        "      def forward(self, x):\n",
        "          b, num_tokens, d_in = x.shape\n",
        "\n",
        "          keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "          queries = self.W_query(x)\n",
        "          values = self.W_value(x)\n",
        "\n",
        "          # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "          # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "          keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "          values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "          queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "          # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "          keys = keys.transpose(1, 2)\n",
        "          queries = queries.transpose(1, 2)\n",
        "          values = values.transpose(1, 2)\n",
        "\n",
        "          # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "          attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "          # Original mask truncated to the number of tokens and converted to boolean\n",
        "          mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "          # Use the mask to fill attention scores\n",
        "          attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "          attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "          attn_weights = self.dropout(  attn_weights)\n",
        "\n",
        "          # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "          context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "          # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "          context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "          context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "          return context_vec\n",
        "\n",
        "    class TransformerBlock(nn.Module):\n",
        "        def __init__(self, cfg):\n",
        "            super().__init__()\n",
        "            self.att = GPTModel.MultiHeadAttention(\n",
        "                d_in=cfg[\"emb_dim\"],\n",
        "                d_out=cfg[\"emb_dim\"],\n",
        "                context_length=cfg[\"context_length\"],\n",
        "                num_heads=cfg[\"n_heads\"],\n",
        "                dropout=cfg[\"drop_rate\"],\n",
        "                qkv_bias=cfg[\"qkv_bias\"])\n",
        "            self.ff = GPTModel.FeedForward(cfg)\n",
        "            self.norm1 = GPTModel.LayerNorm(cfg[\"emb_dim\"])\n",
        "            self.norm2 = GPTModel.LayerNorm(cfg[\"emb_dim\"])\n",
        "            self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        def forward(self, x):\n",
        "            # Shortcut connection for attention block\n",
        "            shortcut = x\n",
        "            x = self.norm1(x)\n",
        "            x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
        "            x = self.drop_shortcut(x)\n",
        "            x = x + shortcut  # Add the original input back\n",
        "\n",
        "            # Shortcut connection for feed forward block\n",
        "            shortcut = x\n",
        "            x = self.norm2(x)\n",
        "            x = self.ff(x)\n",
        "            x = self.drop_shortcut(x)\n",
        "            x = x + shortcut  # Add the original input back\n",
        "\n",
        "            return x\n",
        "\n",
        "    # GPTModel itself\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        # Use a placeholder for TransformerBlock\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[GPTModel.TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        # Use a placeholder for LayerNorm\n",
        "        self.final_norm = GPTModel.LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds\n",
        "\n",
        "        # dropout helps in generalization and avoid overfitting\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n",
        "\n",
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx\n",
        "\n"
      ],
      "metadata": {
        "id": "qnk-HptmYRF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenUtils:\n",
        "  @staticmethod\n",
        "  def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "  @staticmethod\n",
        "  def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())\n"
      ],
      "metadata": {
        "id": "yXyUpMKDYudX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class GPT2Weightloader:\n",
        "\n",
        "  @staticmethod\n",
        "  def download_and_load_gpt2(model_size, models_dir):\n",
        "    # Validate model size\n",
        "    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n",
        "    if model_size not in allowed_sizes:\n",
        "        raise ValueError(f\"Model size not in {allowed_sizes}\")\n",
        "\n",
        "    # Define paths\n",
        "    model_dir = os.path.join(models_dir, model_size)\n",
        "    base_url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
        "    backup_base_url = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2\"\n",
        "    filenames = [\n",
        "        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n",
        "        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n",
        "        \"model.ckpt.meta\", \"vocab.bpe\"\n",
        "    ]\n",
        "\n",
        "    # Download files\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    for filename in filenames:\n",
        "        file_url = os.path.join(base_url, model_size, filename)\n",
        "        backup_url = os.path.join(backup_base_url, model_size, filename)\n",
        "        file_path = os.path.join(model_dir, filename)\n",
        "        GPT2Weightloader.download_file(file_url, file_path, backup_url)\n",
        "\n",
        "    # Load settings and params\n",
        "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
        "    settings = json.load(open(os.path.join(model_dir, \"hparams.json\"), \"r\", encoding=\"utf-8\"))\n",
        "    params = GPT2Weightloader.load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n",
        "\n",
        "    return settings, params\n",
        "\n",
        "  @staticmethod\n",
        "  def download_file(url, destination, backup_url=None):\n",
        "    def _attempt_download(download_url):\n",
        "        response = requests.get(download_url, stream=True, timeout=60)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        file_size = int(response.headers.get(\"Content-Length\", 0))\n",
        "\n",
        "        # Check if file exists and has same size\n",
        "        if os.path.exists(destination):\n",
        "            file_size_local = os.path.getsize(destination)\n",
        "            if file_size and file_size == file_size_local:\n",
        "                print(f\"File already exists and is up-to-date: {destination}\")\n",
        "                return True\n",
        "\n",
        "        block_size = 1024  # 1 KB\n",
        "        desc = os.path.basename(download_url)\n",
        "        with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=desc) as progress_bar:\n",
        "            with open(destination, \"wb\") as file:\n",
        "                for chunk in response.iter_content(chunk_size=block_size):\n",
        "                    if chunk:\n",
        "                        file.write(chunk)\n",
        "                        progress_bar.update(len(chunk))\n",
        "        return True\n",
        "\n",
        "    try:\n",
        "        if _attempt_download(url):\n",
        "            return\n",
        "    except requests.exceptions.RequestException:\n",
        "        if backup_url is not None:\n",
        "            print(f\"Primary URL ({url}) failed. Attempting backup URL: {backup_url}\")\n",
        "            try:\n",
        "                if _attempt_download(backup_url):\n",
        "                    return\n",
        "            except requests.exceptions.RequestException:\n",
        "                pass\n",
        "\n",
        "        error_message = (\n",
        "            f\"Failed to download from both primary URL ({url})\"\n",
        "            f\"{' and backup URL (' + backup_url + ')' if backup_url else ''}.\"\n",
        "            \"\\nCheck your internet connection or the file availability.\\n\"\n",
        "            \"For help, visit: https://github.com/rasbt/LLMs-from-scratch/discussions/273\"\n",
        "        )\n",
        "        print(error_message)\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "  # Alternative way using `requests`\n",
        "  \"\"\"\n",
        "  def download_file(url, destination):\n",
        "      # Send a GET request to download the file in streaming mode\n",
        "      response = requests.get(url, stream=True)\n",
        "\n",
        "      # Get the total file size from headers, defaulting to 0 if not present\n",
        "      file_size = int(response.headers.get(\"content-length\", 0))\n",
        "\n",
        "      # Check if file exists and has the same size\n",
        "      if os.path.exists(destination):\n",
        "          file_size_local = os.path.getsize(destination)\n",
        "          if file_size == file_size_local:\n",
        "              print(f\"File already exists and is up-to-date: {destination}\")\n",
        "              return\n",
        "\n",
        "      # Define the block size for reading the file\n",
        "      block_size = 1024  # 1 Kilobyte\n",
        "\n",
        "      # Initialize the progress bar with total file size\n",
        "      progress_bar_description = url.split(\"/\")[-1]  # Extract filename from URL\n",
        "      with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=progress_bar_description) as progress_bar:\n",
        "          # Open the destination file in binary write mode\n",
        "          with open(destination, \"wb\") as file:\n",
        "              # Iterate over the file data in chunks\n",
        "              for chunk in response.iter_content(block_size):\n",
        "                  progress_bar.update(len(chunk))  # Update progress bar\n",
        "                  file.write(chunk)  # Write the chunk to the file\n",
        "  \"\"\"\n",
        "\n",
        "  @staticmethod\n",
        "  def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n",
        "    # Initialize parameters dictionary with empty blocks for each layer\n",
        "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
        "\n",
        "    # Iterate over each variable in the checkpoint\n",
        "    for name, _ in tf.train.list_variables(ckpt_path):\n",
        "        # Load the variable and remove singleton dimensions\n",
        "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
        "\n",
        "        # Process the variable name to extract relevant parts\n",
        "        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
        "\n",
        "        # Identify the target dictionary for the variable\n",
        "        target_dict = params\n",
        "        if variable_name_parts[0].startswith(\"h\"):\n",
        "            layer_number = int(variable_name_parts[0][1:])\n",
        "            target_dict = params[\"blocks\"][layer_number]\n",
        "\n",
        "        # Recursively access or create nested dictionaries\n",
        "        for key in variable_name_parts[1:-1]:\n",
        "            target_dict = target_dict.setdefault(key, {})\n",
        "\n",
        "        # Assign the variable array to the last key\n",
        "        last_key = variable_name_parts[-1]\n",
        "        target_dict[last_key] = variable_array\n",
        "\n",
        "    return params\n",
        "\n",
        "  @staticmethod\n",
        "  def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))\n",
        "\n",
        "  @staticmethod\n",
        "  def load_weights_into_gpt(gpt, params):\n",
        "      gpt.pos_emb.weight = GPT2Weightloader.assign(gpt.pos_emb.weight, params['wpe'])\n",
        "      gpt.tok_emb.weight = GPT2Weightloader.assign(gpt.tok_emb.weight, params['wte'])\n",
        "\n",
        "      for b in range(len(params[\"blocks\"])):\n",
        "          q_w, k_w, v_w = np.split(\n",
        "              (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "          gpt.trf_blocks[b].att.W_query.weight = GPT2Weightloader.assign(\n",
        "              gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
        "          gpt.trf_blocks[b].att.W_key.weight = GPT2Weightloader.assign(\n",
        "              gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
        "          gpt.trf_blocks[b].att.W_value.weight = GPT2Weightloader.assign(\n",
        "              gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
        "\n",
        "          q_b, k_b, v_b = np.split(\n",
        "              (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "          gpt.trf_blocks[b].att.W_query.bias = GPT2Weightloader.assign(\n",
        "              gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
        "          gpt.trf_blocks[b].att.W_key.bias = GPT2Weightloader.assign(\n",
        "              gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
        "          gpt.trf_blocks[b].att.W_value.bias = GPT2Weightloader.assign(\n",
        "              gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
        "\n",
        "          gpt.trf_blocks[b].att.out_proj.weight = GPT2Weightloader.assign(\n",
        "              gpt.trf_blocks[b].att.out_proj.weight,\n",
        "              params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "          gpt.trf_blocks[b].att.out_proj.bias = GPT2Weightloader.assign(\n",
        "              gpt.trf_blocks[b].att.out_proj.bias,\n",
        "              params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "          gpt.trf_blocks[b].ff.layers[0].weight = GPT2Weightloader.assign(\n",
        "              gpt.trf_blocks[b].ff.layers[0].weight,\n",
        "              params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "          gpt.trf_blocks[b].ff.layers[0].bias = GPT2Weightloader.assign(\n",
        "              gpt.trf_blocks[b].ff.layers[0].bias,\n",
        "              params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "          gpt.trf_blocks[b].ff.layers[2].weight = GPT2Weightloader.assign(\n",
        "              gpt.trf_blocks[b].ff.layers[2].weight,\n",
        "              params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "          gpt.trf_blocks[b].ff.layers[2].bias = GPT2Weightloader.assign(\n",
        "              gpt.trf_blocks[b].ff.layers[2].bias,\n",
        "              params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "          gpt.trf_blocks[b].norm1.scale = GPT2Weightloader.assign(\n",
        "              gpt.trf_blocks[b].norm1.scale,\n",
        "              params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "          gpt.trf_blocks[b].norm1.shift = GPT2Weightloader.assign(\n",
        "              gpt.trf_blocks[b].norm1.shift,\n",
        "              params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "          gpt.trf_blocks[b].norm2.scale = GPT2Weightloader.assign(\n",
        "              gpt.trf_blocks[b].norm2.scale,\n",
        "              params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "          gpt.trf_blocks[b].norm2.shift = GPT2Weightloader.assign(\n",
        "              gpt.trf_blocks[b].norm2.shift,\n",
        "              params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "      gpt.final_norm.scale = GPT2Weightloader.assign(gpt.final_norm.scale, params[\"g\"])\n",
        "      gpt.final_norm.shift = GPT2Weightloader.assign(gpt.final_norm.shift, params[\"b\"])\n",
        "      gpt.out_head.weight = GPT2Weightloader.assign(gpt.out_head.weight, params[\"wte\"])\n"
      ],
      "metadata": {
        "id": "TyejSWTPYKRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load pretrained LLM\n",
        "\n",
        "\n",
        "Instead of using the smallest 124 million parameter model as before, we load the medium-sized model with 355 million parameters.\n",
        "\n",
        "The reason for this choice is that the 124 million parameter model is too limited in capacity to achieve qualitatively satisfactory results via instruction finetuning."
      ],
      "metadata": {
        "id": "hsVG4K5zZdnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,     # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"drop_rate\": 0.0,        # Dropout rate\n",
        "    \"qkv_bias\": True         # Query-key-value bias\n",
        "}\n",
        "\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
        "\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
        "\n",
        "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
        "settings, params = GPT2Weightloader.download_and_load_gpt2(\n",
        "    model_size=model_size,\n",
        "    models_dir=\"gpt2\"\n",
        ")\n",
        "\n",
        "model = GPTModel(BASE_CONFIG)\n",
        "GPT2Weightloader.load_weights_into_gpt(model, params)\n",
        "model.eval();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0uJSPu5ZBwP",
        "outputId": "6f1cf755-9a1d-4291-d090-962c1a5bfdf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 243kiB/s]\n",
            "encoder.json: 100%|██████████| 1.04M/1.04M [00:01<00:00, 558kiB/s]\n",
            "hparams.json: 100%|██████████| 91.0/91.0 [00:00<00:00, 153kiB/s]\n",
            "model.ckpt.data-00000-of-00001: 100%|██████████| 1.42G/1.42G [10:03<00:00, 2.35MiB/s]\n",
            "model.ckpt.index: 100%|██████████| 10.4k/10.4k [00:00<00:00, 15.1MiB/s]\n",
            "model.ckpt.meta: 100%|██████████| 927k/927k [00:01<00:00, 516kiB/s]\n",
            "vocab.bpe: 100%|██████████| 456k/456k [00:01<00:00, 368kiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test result on pretrained LLM and assess the performance."
      ],
      "metadata": {
        "id": "qtTedC80ZuLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_input(entry):\n",
        "      instruction_text = (\n",
        "          f\"Below is an instruction that describes a task. \"\n",
        "          f\"Write a response that appropriately completes the request.\"\n",
        "          f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
        "      )\n",
        "\n",
        "      input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
        "      return instruction_text + input_text\n",
        "\n",
        "torch.manual_seed(123)\n",
        "torch.cuda.manual_seed(123)\n",
        "input_text = format_input(val_data[0])\n",
        "print(input_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIoM9OcGZmyo",
        "outputId": "2c704a18-3b49-40e9-e082-f14296346d90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=TokenUtils.text_to_token_ids(input_text, tokenizer),\n",
        "    max_new_tokens=35,\n",
        "    context_size=BASE_CONFIG[\"context_length\"],\n",
        "    eos_id=50256,\n",
        ")\n",
        "generated_text = TokenUtils.token_ids_to_text(token_ids, tokenizer)\n",
        "\n",
        "response_text = generated_text[len(input_text):].strip()\n",
        "print(response_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqB0lnmnaJEN",
        "outputId": "f2635daf-d621-46aa-b569-ba8f9202b58b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Response:\n",
            "\n",
            "The chef cooks the meal every day.\n",
            "\n",
            "### Instruction:\n",
            "\n",
            "Convert the active sentence to passive: 'The chef cooks the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from the output, the pretrained model is not yet capable of correctly following the given instruction.\n",
        "\n",
        "While it does create a \"Response\" section, it simply repeats\n",
        "the original input sentence and part of the instruction, failing to convert the active sentence to passive voice as requested.\n",
        "\n"
      ],
      "metadata": {
        "id": "PrVG3DZUcLoN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PREVIOUSLY DEFINED FUNCTIONS WHICH WE WILL REQUIRE"
      ],
      "metadata": {
        "id": "90i9wLgvceF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss\n",
        "\n",
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # Crop current context if it exceeds the supported context size\n",
        "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
        "        # then only the last 5 tokens are used as context\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond) ### batch, n_tokens, vocab_size\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
        "\n",
        "        # Get the idx of the vocab entry with the highest probability value\n",
        "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = TokenUtils.text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "    decoded_text = TokenUtils.token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()\n",
        "\n",
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches\n",
        "\n",
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        generate_and_print_sample(\n",
        "            model, tokenizer, device, start_context\n",
        "        )\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen\n"
      ],
      "metadata": {
        "id": "kzpbeB2Sbt8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine tune with instruction data\n",
        "\n",
        "- We will train our model again with new instruction data.\n",
        "\n",
        "- Why we pretrain the LLM in that case?\n",
        "  - Because now model starts from the knowledgeable state instead of random state.\n",
        "  - It knows many things like semantic meaning of words, etc.\n"
      ],
      "metadata": {
        "id": "141iKaJgeLAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "with torch.no_grad():\n",
        "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z23E0wYge10z",
        "outputId": "ee456e39-f6b5-43f6-a48e-2bb026fede0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 3.825909376144409\n",
            "Validation loss: 3.7619347095489504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "start training on instruction data."
      ],
      "metadata": {
        "id": "ms6jD0eqfE58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "torch.cuda.manual_seed(123)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnTTLgWve_Rc",
        "outputId": "12ead68c-9d3a-416f-973f-ecfb21c53cd7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 2.637, Val loss 2.626\n",
            "Ep 1 (Step 000005): Train loss 1.174, Val loss 1.103\n",
            "Ep 1 (Step 000010): Train loss 0.872, Val loss 0.944\n",
            "Ep 1 (Step 000015): Train loss 0.857, Val loss 0.906\n",
            "Ep 1 (Step 000020): Train loss 0.776, Val loss 0.881\n",
            "Ep 1 (Step 000025): Train loss 0.754, Val loss 0.859\n",
            "Ep 1 (Step 000030): Train loss 0.799, Val loss 0.836\n",
            "Ep 1 (Step 000035): Train loss 0.714, Val loss 0.808\n",
            "Ep 1 (Step 000040): Train loss 0.672, Val loss 0.806\n",
            "Ep 1 (Step 000045): Train loss 0.633, Val loss 0.789\n",
            "Ep 1 (Step 000050): Train loss 0.663, Val loss 0.783\n",
            "Ep 1 (Step 000055): Train loss 0.760, Val loss 0.763\n",
            "Ep 1 (Step 000060): Train loss 0.719, Val loss 0.743\n",
            "Ep 1 (Step 000065): Train loss 0.653, Val loss 0.735\n",
            "Ep 1 (Step 000070): Train loss 0.533, Val loss 0.729\n",
            "Ep 1 (Step 000075): Train loss 0.568, Val loss 0.729\n",
            "Ep 1 (Step 000080): Train loss 0.604, Val loss 0.725\n",
            "Ep 1 (Step 000085): Train loss 0.509, Val loss 0.710\n",
            "Ep 1 (Step 000090): Train loss 0.563, Val loss 0.691\n",
            "Ep 1 (Step 000095): Train loss 0.502, Val loss 0.681\n",
            "Ep 1 (Step 000100): Train loss 0.504, Val loss 0.677\n",
            "Ep 1 (Step 000105): Train loss 0.565, Val loss 0.670\n",
            "Ep 1 (Step 000110): Train loss 0.554, Val loss 0.666\n",
            "Ep 1 (Step 000115): Train loss 0.508, Val loss 0.663\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is prepared every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive:\n",
            "Ep 2 (Step 000120): Train loss 0.435, Val loss 0.671\n",
            "Ep 2 (Step 000125): Train loss 0.451, Val loss 0.687\n",
            "Ep 2 (Step 000130): Train loss 0.447, Val loss 0.682\n",
            "Ep 2 (Step 000135): Train loss 0.405, Val loss 0.682\n",
            "Ep 2 (Step 000140): Train loss 0.410, Val loss 0.681\n",
            "Ep 2 (Step 000145): Train loss 0.369, Val loss 0.681\n",
            "Ep 2 (Step 000150): Train loss 0.382, Val loss 0.675\n",
            "Ep 2 (Step 000155): Train loss 0.414, Val loss 0.675\n",
            "Ep 2 (Step 000160): Train loss 0.412, Val loss 0.684\n",
            "Ep 2 (Step 000165): Train loss 0.379, Val loss 0.686\n",
            "Ep 2 (Step 000170): Train loss 0.322, Val loss 0.680\n",
            "Ep 2 (Step 000175): Train loss 0.338, Val loss 0.667\n",
            "Ep 2 (Step 000180): Train loss 0.392, Val loss 0.656\n",
            "Ep 2 (Step 000185): Train loss 0.414, Val loss 0.657\n",
            "Ep 2 (Step 000190): Train loss 0.340, Val loss 0.648\n",
            "Ep 2 (Step 000195): Train loss 0.328, Val loss 0.633\n",
            "Ep 2 (Step 000200): Train loss 0.309, Val loss 0.633\n",
            "Ep 2 (Step 000205): Train loss 0.353, Val loss 0.631\n",
            "Ep 2 (Step 000210): Train loss 0.364, Val loss 0.630\n",
            "Ep 2 (Step 000215): Train loss 0.393, Val loss 0.634\n",
            "Ep 2 (Step 000220): Train loss 0.297, Val loss 0.644\n",
            "Ep 2 (Step 000225): Train loss 0.342, Val loss 0.658\n",
            "Ep 2 (Step 000230): Train loss 0.294, Val loss 0.656\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is cooked every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: What is the capital of the United Kingdom\n",
            "Ep 3 (Step 000235): Train loss 0.329, Val loss 0.663\n",
            "Ep 3 (Step 000240): Train loss 0.280, Val loss 0.697\n",
            "Ep 3 (Step 000245): Train loss 0.274, Val loss 0.705\n",
            "Ep 3 (Step 000250): Train loss 0.248, Val loss 0.687\n",
            "Ep 3 (Step 000255): Train loss 0.273, Val loss 0.674\n",
            "Ep 3 (Step 000260): Train loss 0.268, Val loss 0.679\n",
            "Ep 3 (Step 000265): Train loss 0.279, Val loss 0.702\n",
            "Ep 3 (Step 000270): Train loss 0.279, Val loss 0.717\n",
            "Ep 3 (Step 000275): Train loss 0.273, Val loss 0.711\n",
            "Ep 3 (Step 000280): Train loss 0.291, Val loss 0.722\n",
            "Ep 3 (Step 000285): Train loss 0.294, Val loss 0.718\n",
            "Ep 3 (Step 000290): Train loss 0.292, Val loss 0.708\n",
            "Ep 3 (Step 000295): Train loss 0.265, Val loss 0.698\n",
            "Ep 3 (Step 000300): Train loss 0.266, Val loss 0.684\n",
            "Ep 3 (Step 000305): Train loss 0.270, Val loss 0.683\n",
            "Ep 3 (Step 000310): Train loss 0.266, Val loss 0.695\n",
            "Ep 3 (Step 000315): Train loss 0.237, Val loss 0.696\n",
            "Ep 3 (Step 000320): Train loss 0.254, Val loss 0.687\n",
            "Ep 3 (Step 000325): Train loss 0.236, Val loss 0.695\n",
            "Ep 3 (Step 000330): Train loss 0.234, Val loss 0.696\n",
            "Ep 3 (Step 000335): Train loss 0.231, Val loss 0.695\n",
            "Ep 3 (Step 000340): Train loss 0.244, Val loss 0.690\n",
            "Ep 3 (Step 000345): Train loss 0.246, Val loss 0.681\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The chef cooks the meal every day.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: What is the capital of the United Kingdom? \n",
            "Ep 4 (Step 000350): Train loss 0.250, Val loss 0.682\n",
            "Ep 4 (Step 000355): Train loss 0.213, Val loss 0.727\n",
            "Ep 4 (Step 000360): Train loss 0.220, Val loss 0.767\n",
            "Ep 4 (Step 000365): Train loss 0.236, Val loss 0.744\n",
            "Ep 4 (Step 000370): Train loss 0.279, Val loss 0.718\n",
            "Ep 4 (Step 000375): Train loss 0.239, Val loss 0.710\n",
            "Ep 4 (Step 000380): Train loss 0.206, Val loss 0.714\n",
            "Ep 4 (Step 000385): Train loss 0.230, Val loss 0.720\n",
            "Ep 4 (Step 000390): Train loss 0.213, Val loss 0.719\n",
            "Ep 4 (Step 000395): Train loss 0.207, Val loss 0.716\n",
            "Ep 4 (Step 000400): Train loss 0.222, Val loss 0.709\n",
            "Ep 4 (Step 000405): Train loss 0.212, Val loss 0.709\n",
            "Ep 4 (Step 000410): Train loss 0.201, Val loss 0.704\n",
            "Ep 4 (Step 000415): Train loss 0.210, Val loss 0.697\n",
            "Ep 4 (Step 000420): Train loss 0.207, Val loss 0.691\n",
            "Ep 4 (Step 000425): Train loss 0.200, Val loss 0.696\n",
            "Ep 4 (Step 000430): Train loss 0.211, Val loss 0.710\n",
            "Ep 4 (Step 000435): Train loss 0.196, Val loss 0.724\n",
            "Ep 4 (Step 000440): Train loss 0.188, Val loss 0.724\n",
            "Ep 4 (Step 000445): Train loss 0.189, Val loss 0.709\n",
            "Ep 4 (Step 000450): Train loss 0.186, Val loss 0.696\n",
            "Ep 4 (Step 000455): Train loss 0.194, Val loss 0.705\n",
            "Ep 4 (Step 000460): Train loss 0.192, Val loss 0.709\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is cooked by the chef every day.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: What is the opposite of 'quiet'?\n",
            "Ep 5 (Step 000465): Train loss 0.203, Val loss 0.704\n",
            "Ep 5 (Step 000470): Train loss 0.192, Val loss 0.725\n",
            "Ep 5 (Step 000475): Train loss 0.199, Val loss 0.755\n",
            "Ep 5 (Step 000480): Train loss 0.193, Val loss 0.784\n",
            "Ep 5 (Step 000485): Train loss 0.186, Val loss 0.793\n",
            "Ep 5 (Step 000490): Train loss 0.195, Val loss 0.755\n",
            "Ep 5 (Step 000495): Train loss 0.206, Val loss 0.724\n",
            "Ep 5 (Step 000500): Train loss 0.181, Val loss 0.720\n",
            "Ep 5 (Step 000505): Train loss 0.210, Val loss 0.733\n",
            "Ep 5 (Step 000510): Train loss 0.184, Val loss 0.751\n",
            "Ep 5 (Step 000515): Train loss 0.173, Val loss 0.751\n",
            "Ep 5 (Step 000520): Train loss 0.189, Val loss 0.746\n",
            "Ep 5 (Step 000525): Train loss 0.198, Val loss 0.746\n",
            "Ep 5 (Step 000530): Train loss 0.188, Val loss 0.739\n",
            "Ep 5 (Step 000535): Train loss 0.153, Val loss 0.746\n",
            "Ep 5 (Step 000540): Train loss 0.190, Val loss 0.767\n",
            "Ep 5 (Step 000545): Train loss 0.175, Val loss 0.763\n",
            "Ep 5 (Step 000550): Train loss 0.189, Val loss 0.756\n",
            "Ep 5 (Step 000555): Train loss 0.184, Val loss 0.749\n",
            "Ep 5 (Step 000560): Train loss 0.185, Val loss 0.751\n",
            "Ep 5 (Step 000565): Train loss 0.169, Val loss 0.750\n",
            "Ep 5 (Step 000570): Train loss 0.170, Val loss 0.756\n",
            "Ep 5 (Step 000575): Train loss 0.173, Val loss 0.760\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is cooked every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: What is the capital of the United Kingdom\n",
            "Training completed in 7.95 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "As we can see based on the outputs above, the model trains well, as we can tell based on the decreasing training loss and validation loss values.\n",
        "    \n",
        "Furthermore, based on the response text printed after each epoch, we can see that the model almost correctly follows the instruction to convert the input sentence 'The chef cooks the meal every day.' into passive voice 'The meal is prepared every day by the chef.' (We will properly format and evaluate the responses in a later section.\n",
        "\n",
        "To get better results, we need to finetune the model for more epochs.\n",
        "\n",
        "Finally, let's take a look at the training and validation loss curves"
      ],
      "metadata": {
        "id": "DLUY2wNnf3oK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
        "\n",
        "    # Create a second x-axis for tokens seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(\"loss-plot.pdf\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "N3NpeEiqfVlw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ],
      "metadata": {
        "id": "qejc18ktgUcB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "9691ae0c-cd6c-4582-ff34-556ea1957a9a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see in the loss plot shown above, the model's performance on both the training and validation sets improves substantially over the course of training.\n",
        "\n",
        "The rapid decrease in losses during the initial phase indicates that the model is quickly learning meaningful patterns and representations from the data. Then, as training progresses to the\n",
        "second epoch, the losses continue to decrease but at a slower rate, suggesting that the model is finetuning its learned representations and converging to a stable solution.\n"
      ],
      "metadata": {
        "id": "aU2YNmFTgeCu"
      }
    }
  ]
}