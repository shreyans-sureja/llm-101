{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUowR5c8eeDySNqDnyxfar",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyans-sureja/llm-101/blob/main/part6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Data Pre-processing\n",
        "\n",
        "4 steps:\n",
        "\n",
        "1. Tokenization\n",
        "    * Word based\n",
        "    * Character based\n",
        "    * Sub word based (BPE tokenizer)\n",
        "2. Token Embeddings - converting token Ids to vectors\n",
        "3. Positional Embeddings - encoding position information\n",
        "4. Input Embeddings -\n",
        "   * Token Embeddings + Positional Embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "bA49DY5xfBXn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention Mechanism"
      ],
      "metadata": {
        "id": "hhBHda1IfHBW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what are we trying to solve here?**\n",
        "\n",
        "\"The cat that was sitting on the mat, which was next to the dog, jumped\"\n",
        "\n",
        "complext sentence, LLM should understood and give attention to jumped part in cat context. If we don't introduce attention mechanism LLM may confuse with dog or do not capture assence in long sequence sentences."
      ],
      "metadata": {
        "id": "L7MVfV9EwXN_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 4 types of attention mechanisms\n",
        "\n",
        "1. Simplified self attention\n",
        "2. Self attention\n",
        "3. Causal attention\n",
        "4. Multi-head attention"
      ],
      "metadata": {
        "id": "DpXB9nrcxffR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## History\n",
        "\n",
        "The problem with modeling long sequences, what is the problem with architectures without the attention mechanism whcih came before LLMs?\n",
        "\n",
        "Langugae translation model: Word by word translation does not work, grammatically incorrect sentence.\n",
        "\n",
        "**The translation process requires contextual understanding and grammar alignment.**\n",
        "\n",
        "To address this issue that we can not just use normal neural network as it does not have the memory. And models need the memory to keep the context of previous sentence.  Need in all the requirements like summarization, translation, etc.\n",
        "\n",
        "\n",
        "To address this issue, people broke this into two submodules.\n",
        "- Encoder: Reads and process text\n",
        "- Decoder: Translates text\n",
        "\n",
        "See animation to understand this - https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\n",
        "\n",
        "Before transformers, Recurrent Neural networks(RNNs) were the most popular encoder-decoder architecture for language translation.\n",
        "\n",
        "RNN: output from previous step is fed as input to current text.\n",
        "\n",
        "Stages:\n",
        "1. Input text\n",
        "2. encoder (process input text sequentially)\n",
        "3. update hidden state at each step (internal values at hidden layers)\n",
        "4. final hidden state (encoder tries to capture sentence meaning)\n",
        "5. decoder uses this final hidden state to generate translated sentence (one word at a time)\n"
      ],
      "metadata": {
        "id": "oFUMfYqLyHty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Question - We now have RNNs and it works with sequence to sequence translation then why we need attention mechanism?\n",
        "\n",
        "Big problem with RNNs\n",
        "- Model does not have access to previous hidden states. Decoder has only access of final hidden state. If we have long sequence especially complex sentences this leads to loss of context where dependencies might span long distance.\n",
        "\n",
        "**Encoder compresses entire input sequence into a single hidden state vector, if the sequence is very long, it becomes very difficult for the RNN to capture all information in a single vector.**\n",
        "\n",
        "This is called loss of context which was the biggest problem in RNN before attention mechanism born.\n",
        "\n",
        "Rnns work fine for translating short sentences but does not work for long texts as they don't have direct access to previous words in the input."
      ],
      "metadata": {
        "id": "K4W_WPDkU1B7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------------------------------------------"
      ],
      "metadata": {
        "id": "HyWG40xHoouY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is when attention mechanism came into picture. In 2014, researchers developed the \"Bahdanau attention mechanism for RNNs\".\n",
        "\n",
        "It modifies the encoder-decoder RNN such that decoder can selectively access different parts of the input sequence at each decoding step.\n",
        "\n",
        "When generating an output token, the model has a way to access to all input tokens. It has attention weights which describes how much weight we put to each input token.\n",
        "\n",
        "Transformer archieteture inspired from this.\n",
        "\n",
        "**Summary** - At each decoding step, the model can look back at the entire input sequence and decide which parts are most relevant to generate current word.\n",
        "\n",
        "Dynamic focus on different parts of input sequence allows models to learn long range dependencies more effectively."
      ],
      "metadata": {
        "id": "k0ZpF9O5nM75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self Attention\n",
        "\n",
        "- self attention is a mechanism that allows each position of the input sequence to attend to all the position in the same sequence when computing the respresentation of sequence.\n",
        "\n",
        "- self attention is a key component of contemporary LLMs based on transformer architecture, such as the GPT.\n",
        "\n",
        "- In self attention, the self refers to the mechanisms' ability to compute attention weights by relating different positions in a single input sequence. It learns the relationship b/w various parts of the input itself.\n",
        "\n",
        "- This is in contrast to traditional attention mechanisms where the focus is on relationships b/w elements of 2 different sequences."
      ],
      "metadata": {
        "id": "V5OLVOwn3I1y"
      }
    }
  ]
}