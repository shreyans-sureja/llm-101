{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOztrwKe0a53sOLSxxPr82O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyans-sureja/llm-101/blob/main/part12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer Normalization"
      ],
      "metadata": {
        "id": "52xlf0g6YZYX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "transformer block is stack of\n",
        "\n",
        "1. Layer Normalization\n",
        "2. Masked multi-head attention\n",
        "3. Dropout\n",
        "4. Shortcut connection\n"
      ],
      "metadata": {
        "id": "G3wz8xLtYwRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Why layer normalization?**\n",
        "\n",
        "- Training deep neural networks with many layers can be challenging due to vanishing/exploding gradients.\n",
        "- This leads to unstable training dynamics.\n",
        "- Layer normalization improves the stability and efficiency of neural network training.\n",
        "- Main idea: Adjust outputs of neural network to have mean zero and variance one.\n"
      ],
      "metadata": {
        "id": "GS5QI_y0ZH8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In other words,\n",
        "\n",
        "If layer output is too large or small, gradient magnitudes can become too large or small. This affects training and layer normalization keeps gradient stable.\n",
        "\n",
        "As the training proceeds, input to each layer change(internal covariate shifts), this delays convergence and layer normalization prevents this."
      ],
      "metadata": {
        "id": "-A2qay0pbzu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In GPT-2 and morden transformer architectures, layer normalization is typically applied before and after the multi-head attention module and before the final output layer.\n"
      ],
      "metadata": {
        "id": "qqtuKMa_fF1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Simple example"
      ],
      "metadata": {
        "id": "ynVDQUlNf7qQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoxFKDmTYTfa",
        "outputId": "a1ca9a2e-244b-4d9d-8b0d-2e3d6cc3fbb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_example:  tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969],\n",
            "        [ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085]])\n",
            "output:  tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
            "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
            "       grad_fn=<ReluBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(123)\n",
        "batch_example = torch.randn(2,5)\n",
        "print(\"batch_example: \", batch_example)\n",
        "layer = nn.Sequential(nn.Linear(5,6), nn.ReLU())\n",
        "out = layer(batch_example)\n",
        "print(\"output: \", out)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The neural network layer we have coded consists of a Linear layer followed by a non-linear\n",
        "activation function, ReLU (short for Rectified Linear Unit), which is a standard activation\n",
        "function in neural networks.\n",
        "\n",
        "If you are unfamiliar with ReLU, it simply thresholds negative\n",
        "inputs to 0, ensuring that a layer outputs only positive values, which explains why the\n",
        "resulting layer output does not contain any negative values.\n"
      ],
      "metadata": {
        "id": "jX9-Okr_hYnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can apply layer normalization"
      ],
      "metadata": {
        "id": "M4fokESih0_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean = out.mean(dim=-1, keepdim=True)\n",
        "print(\"mean: \", mean)\n",
        "\n",
        "var = out.var(dim=-1, keepdim=True)\n",
        "print(\"var:\", var)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fod-goOYgWOF",
        "outputId": "2afd633b-7066-489b-8de2-192a001f9b9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean:  tensor([[0.1324],\n",
            "        [0.2170]], grad_fn=<MeanBackward1>)\n",
            "var: tensor([[0.0231],\n",
            "        [0.0398]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using keepdim=True in operations like mean or variance calculation ensures that the\n",
        "output tensor retains the same number of dimensions as the input tensor, even though the\n",
        "operation reduces the tensor along the dimension specified via dim.\n",
        "\n",
        "For instance, without\n",
        "keepdim=True, the returned mean tensor would be a 2-dimensional vector [0.1324,\n",
        "0.2170] instead of a 2Ã—1-dimensional matrix [[0.1324], [0.2170]].\n",
        "\n",
        "For a 2D tensor (like a matrix), using dim=-1 for operations such as\n",
        "mean or variance calculation is the same as using dim=1.\n",
        "\n",
        "This is because -1 refers to the\n",
        "tensor's last dimension, which corresponds to the columns in a 2D tensor.\n",
        "\n",
        "Later, when\n",
        "adding layer normalization to the GPT model, which produces 3D tensors with shape\n",
        "[batch_size, num_tokens, embedding_size], we can still use dim=-1 for normalization\n",
        "across the last dimension, avoiding a change from dim=1 to dim=2.\n"
      ],
      "metadata": {
        "id": "NVhkPoH4jMaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out_norm = (out- mean) / torch.sqrt(var)\n",
        "mean = out_norm.mean(dim=-1, keepdim=True)\n",
        "var = out_norm.var(dim=-1, keepdim=True)\n",
        "\n",
        "print(\"Normalized layer outputs:\\n\", out_norm)\n",
        "print(\"Mean:\\n\", mean)\n",
        "print(\"Variance:\\n\", var)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErK2QoawiTqM",
        "outputId": "58e5228e-9434-4dd4-c429-75bd06534343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized layer outputs:\n",
            " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
            "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
            "       grad_fn=<DivBackward0>)\n",
            "Mean:\n",
            " tensor([[9.9341e-09],\n",
            "        [1.9868e-08]], grad_fn=<MeanBackward1>)\n",
            "Variance:\n",
            " tensor([[1.0000],\n",
            "        [1.0000]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This specific implementation of layer Normalization operates on the last dimension of the\n",
        "input tensor x, which represents the embedding dimension (emb_dim).\n",
        "\n",
        "- The variable eps is a small constant (epsilon) added to the variance to prevent division by zero during normalization.\n",
        "\n",
        "- The scale and shift are two trainable parameters (of the same dimension\n",
        "as the input) that the LLM automatically adjusts during training if it is determined that doing so would improve the model's performance on its training task.\n",
        "\n",
        "- This allows the model to learn appropriate scaling and shifting that best suit the data it is processing."
      ],
      "metadata": {
        "id": "_8CDxnY7yyYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim=-1, keepdim=True)\n",
        "    var = x.var(dim=-1, keepdim=True, unbiased=True)\n",
        "    norm_x = (x - mean) / torch.sqrt(var + self.eps);\n",
        "\n",
        "    return self.scale * norm_x + self.shift"
      ],
      "metadata": {
        "id": "AcqX045RjGh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In our variance calculation method, we have used unbiased=False, which means in the variance calculation, we divide by the number of inputs n in the variance formula.\n",
        "\n",
        "- This approach does not apply Bessel's correction, which typically uses n-1 instead of n in the denominator to adjust for bias in sample variance estimation.\n",
        "\n",
        "- This decision results in a so-called biased estimate of the variance.\n",
        "\n",
        "- For large-scale language models (LLMs), where the embedding dimension n is significantly large, the difference between using n and n-1 is practically negligible.\n",
        "\n",
        "- We chose this approach to ensure compatibility with the GPT-2 model's normalization layers and because it reflects TensorFlow's default behavior, which was used to implement the original GPT2 model."
      ],
      "metadata": {
        "id": "PbubJiHf0z2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ln = LayerNorm(emb_dim=5)\n",
        "out_ln = ln(batch_example)\n",
        "mean = out_ln.mean(dim=-1, keepdim=True)\n",
        "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
        "print(\"Mean:\\n\", mean)\n",
        "print(\"Variance:\\n\", var)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6qHE0Mjz90c",
        "outputId": "dc689c4d-92b6-46a9-ad8e-86c1af18fcba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean:\n",
            " tensor([[-1.4901e-08],\n",
            "        [ 2.3842e-08]], grad_fn=<MeanBackward1>)\n",
            "Variance:\n",
            " tensor([[0.8000],\n",
            "        [0.8000]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Layer vs Batch Normalization** -\n",
        "\n",
        "**Layer** - Normalize feature dimension independently of batch size.\n",
        "\n",
        "**Batch** - Normalize across the batch dimension and spatial dimensions.\n",
        "\n",
        "Available hardware dictates batch size and hence normalization depends on that, whearas layer normalization is more flexible and stability for distributed training and specially environments which lack resources."
      ],
      "metadata": {
        "id": "4sZwM6hvTZ7T"
      }
    }
  ]
}