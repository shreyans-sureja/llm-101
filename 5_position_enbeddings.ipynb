{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyans-sureja/llm-101/blob/main/part5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIRTbnZvoN-w"
      },
      "source": [
        "# Position Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ktw2grFzoV3K"
      },
      "source": [
        "1. Until now, we looked at token embedding.\n",
        "\n",
        "**Issue** -\n",
        "\n",
        "*   The cat sat on the mat\n",
        "*   On the mat the cat sat\n",
        "\n",
        "\n",
        "2. In the embedding layer, same token ID gets mapped to the same vector representation, regardless of where the token ID is positioned in the input sequence.\n",
        "\n",
        "3. It is helpful to inject additional position information to LLM.\n",
        "\n",
        "There are two types of positional embeddings.\n",
        "\n",
        "1.   Absolute - For each position in input sequence, a unique embedding is added to the token's embedding to convey its exact location. Positional vectors have the same dimension as original token embeddings.\n",
        "2.   Relative - The emphasis is on the relative position or distance b/w tokens. The model learns the relationships in terms of \"how far apart\" rather than at which exact position. \\\n",
        "    - Advantage : model can generalize better to sequence varying lengths, even if it has not seen such lengths during training.\n",
        "    - longer sequence also better in relative positional embeddings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZFizTyadap9"
      },
      "source": [
        "Both types of positional encodings enable LLM to understand the order and relationship b/w tokens, ensuring accurate and context aware predictions.\n",
        "\n",
        "Choice b/w the two depends on specific application and nature of data being processed.\n",
        "\n",
        "Absolute - suitable when fixed order of token is crucial, such as sequence generation (GPT trained on this)\n",
        "\n",
        "Relative - suitable for tasks like language modeling over long sequences, where the same phrase can appear in different parts of the sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "961a5BaHf1tm"
      },
      "source": [
        "# Absolute vs Relative Positional Embeddings\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ Absolute Positional Embeddings\n",
        "**Definition**: Assigns each position in the sequence a unique encoding (added to token embeddings).  \n",
        "- **Fixed (sinusoidal)**: deterministic function of position.  \n",
        "- **Learned**: trainable embedding per position index.\n",
        "\n",
        "### ‚úÖ Pros\n",
        "- Simple to implement.  \n",
        "- Fixed sinusoidal ‚Üí extrapolates to longer sequences.  \n",
        "- Stable and widely adopted in early models (BERT, GPT-2).\n",
        "\n",
        "### ‚ùå Cons\n",
        "- Position is treated as a *global index* (e.g., ‚Äúposition 7‚Äù), not relative to others.  \n",
        "- Learned version fails to generalize beyond max training length.  \n",
        "- Less expressive for local dependencies (e.g., ‚Äútoken just before this one‚Äù).\n",
        "\n",
        "### üìå Where Used\n",
        "- NLP models with bounded input length (BERT, GPT-2).  \n",
        "- Tasks where **absolute position matters** (e.g., machine translation, classification).  \n",
        "\n",
        "---\n",
        "\n",
        "## üîπ Relative Positional Embeddings\n",
        "**Definition**: Encodes *distance between tokens* directly into the attention mechanism.  \n",
        "- Example: ‚Äúthis token is 2 steps behind‚Äù instead of ‚ÄúI‚Äôm at index 7‚Äù.\n",
        "\n",
        "### ‚úÖ Pros\n",
        "- Captures **local order relations** naturally.  \n",
        "- Generalizes better to longer/unseen sequences.  \n",
        "- Strong performance in long-context tasks.  \n",
        "- More robust to shifts in input.  \n",
        "\n",
        "### ‚ùå Cons\n",
        "- More complex to implement (modifies attention calculation).  \n",
        "- Slightly higher compute cost.  \n",
        "- Can overweight nearby positions if not balanced.\n",
        "\n",
        "### üìå Where Used\n",
        "- Long-context or extrapolative tasks (Transformer-XL, T5, DeBERTa, LLaMA).  \n",
        "- Language modeling, document QA, music/audio, protein sequences.  \n",
        "\n",
        "---\n",
        "\n",
        "## üîπ Quick Rule of Thumb\n",
        "- **Absolute** ‚Üí use if you want **simplicity** and your task has a fixed maximum input length.  \n",
        "- **Relative** ‚Üí use if you want **scalability**, **generalization to longer sequences**, or your task is highly dependent on relative order.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpBjz27jgXFj"
      },
      "source": [
        "### Practical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rs0YYulXoTA1"
      },
      "outputs": [],
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PhOHh48kgkrM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GNcvVSdg1iK",
        "outputId": "c7733728-2018-4bfb-8283-7dcb9ed6b4be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding(50257, 256)\n"
          ]
        }
      ],
      "source": [
        "print(token_embedding_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Vc2oDLhChU1U"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pmNJrhXgg-5Q"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pq3pw7wNhbug",
        "outputId": "c5b1b22c-5bed-4000-e53e-44062e806ffb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of characters: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/shreyans-sureja/llm-101/main/data/the-verdict.txt\"\n",
        "response = requests.get(url)\n",
        "raw_text = response.text\n",
        "\n",
        "print(\"Total number of characters:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "jBmt2VfzjC9q"
      },
      "outputs": [],
      "source": [
        "max_length = 4\n",
        "\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=max_length,\n",
        "    stride=max_length, shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWERQFOIkD0f",
        "outputId": "abccddc4-12f9-421b-a0d4-6db6ad0a5dbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token IDs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n"
          ]
        }
      ],
      "source": [
        "print(\"Token IDs:\\n\", inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdIuQ_aUkHMp",
        "outputId": "4a899379-344f-4b86-edee-582295a2be8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 4])\n"
          ]
        }
      ],
      "source": [
        "print(inputs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc6t6X7BkdVN",
        "outputId": "4e24248b-8ff1-4be3-fd5b-f2baf7fc79ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([256])\n"
          ]
        }
      ],
      "source": [
        "print(token_embedding_layer(torch.tensor(1)).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKCD1b6XlLLA",
        "outputId": "bcbcd3fd-92b1-444a-9b82-85ee931301dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ],
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RouUepmHl10k"
      },
      "source": [
        "Now we need to add positional embedding into this token embedding.\n",
        "\n",
        "So we will create another embedding layer for positional encoding.\n",
        "\n",
        "here max_length is 4 so each time only 4 vectors will be processed. so we need [4, 256] positional vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "BeAp64_IlkQd"
      },
      "outputs": [],
      "source": [
        "context_length = max_length\n",
        "positional_embedding_layer = torch.nn.Embedding(max_length, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "La4olPLNmye-",
        "outputId": "a2e24041-98ae-41d7-c90b-215f1a4218a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding(4, 256)\n"
          ]
        }
      ],
      "source": [
        "print(positional_embedding_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Mb7GzrOm46a",
        "outputId": "48b615ad-54fc-47f1-e13f-9eb6d6d0ec6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ],
      "source": [
        "positional_embedding = positional_embedding_layer(torch.arange(max_length))\n",
        "print(positional_embedding.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqs3kj3unpxE",
        "outputId": "fbc09cff-4d2c-4cca-9463-d0f044728cb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-0.4954, -0.0626, -1.9312,  1.2758, -0.4081,  0.5276,  1.0432, -1.1776,\n",
            "         0.8742,  1.9163,  1.0273,  1.2036,  1.9936, -0.3877, -0.7666,  0.1392,\n",
            "        -0.0487, -0.9630, -1.0842,  0.1706,  1.2167,  2.7928,  0.7048, -0.3764,\n",
            "        -0.5912, -0.6708, -0.6660,  0.4840,  0.3936, -1.1552,  1.0938, -1.9101,\n",
            "        -1.0789,  1.6486,  1.5196, -0.3195,  0.4708, -1.7364,  0.0060,  0.1909,\n",
            "         0.4287,  0.9278,  0.6019,  0.5021, -0.8357, -1.1665,  0.4483, -0.8482,\n",
            "        -1.6166,  1.2787, -0.6010, -0.7984,  0.9516, -1.1353,  0.8852,  1.5233,\n",
            "         0.2669, -0.4390, -0.6122,  1.1514,  0.8585,  0.5500,  0.1262,  0.0712,\n",
            "         0.1361, -0.6033,  1.2824,  0.7581,  0.6890, -0.6455, -0.1948, -0.0847,\n",
            "         2.3061,  0.0301,  0.1472, -0.8104,  2.0441, -0.4239, -0.0200, -2.0380,\n",
            "        -0.7801, -1.4706,  0.2495,  0.5418,  0.0195,  1.2017, -1.6508,  1.1710,\n",
            "        -0.1590, -2.2909, -0.1686,  0.5532, -1.3528,  0.0075,  1.4115,  0.7623,\n",
            "         1.6240, -0.1515, -2.0096, -0.1444, -1.2104, -0.8453, -0.9412,  0.4292,\n",
            "        -0.2688,  0.1350, -0.9124, -0.6876,  0.6225, -0.6757, -1.6245, -1.8086,\n",
            "        -0.8431,  1.0566,  1.3727,  0.2727,  2.1141,  0.4863, -1.1120,  0.0153,\n",
            "         0.9904,  0.3432, -0.8739,  0.3342,  0.1435,  1.7068, -0.7826,  1.2433,\n",
            "        -0.1782, -0.3817,  0.7777, -1.7564,  0.0065, -0.0141, -0.7333,  2.0841,\n",
            "         1.2004,  0.4154, -0.1843,  0.6907,  1.5271,  0.5663, -0.6783,  0.0766,\n",
            "         1.5362,  1.8542, -2.0737, -0.4690, -0.3657,  1.1217, -1.5297, -2.1021,\n",
            "         1.2137,  0.2267,  0.4854, -1.0266,  1.5433, -0.0383, -0.7382,  0.5262,\n",
            "        -2.2671, -0.7798, -0.1716, -0.2478,  0.6435,  1.3519,  0.1802,  0.9528,\n",
            "        -0.3723, -2.4566,  0.0681,  0.1612, -0.0192,  0.1289,  1.2862, -0.1929,\n",
            "        -0.2434, -1.0607,  1.0268,  0.1535,  0.6322,  0.8145,  2.4688,  0.7204,\n",
            "         0.1699,  0.4652,  0.2854,  1.3490, -0.0332, -0.4085, -0.0888,  0.4926,\n",
            "         0.7659,  0.4485,  0.1802,  0.3759,  0.3242,  1.9008, -1.2481, -0.3034,\n",
            "         0.1728, -1.1469, -1.6128,  0.9368, -0.4495, -0.4729,  1.2169, -0.6339,\n",
            "        -1.1222,  1.6002, -0.2637, -0.3227,  1.0678, -0.0740,  1.5520,  1.2645,\n",
            "         0.6281,  0.5797, -1.5771,  0.2598,  0.1450,  0.3133, -1.5972, -0.0245,\n",
            "        -1.5809, -0.1941, -1.3320, -1.3959, -0.3774, -0.2108,  0.3710, -0.7894,\n",
            "         0.7681,  0.9315,  0.0433,  0.3829, -0.8630, -2.4105, -0.1975, -1.3195,\n",
            "        -0.3791,  1.4475,  1.3285,  1.5609, -0.0744,  0.9190, -0.6638, -0.6585,\n",
            "        -1.7099,  1.3820,  0.7626,  0.4979,  0.6066,  0.5298, -0.8813,  0.2011],\n",
            "       grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(positional_embedding[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hny40RYMnu8q",
        "outputId": "1b358b73-8a01-40a9-d552-f71ae0672e21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ],
      "source": [
        "# Python broadcasting\n",
        "input_embeddings = token_embeddings + positional_embedding\n",
        "print(input_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IDJ2f28oThc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyM9DAYlSi5qObfBFvqQSn6w",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
